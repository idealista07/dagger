- Research physical memory, virtual memory, pagetables
	* Best way to make a direct physical map
	* How to manipulate the pagetables
	* Allocators!

	1) Implement the pages themselves
		1a) Page Table                   Entry
		1b) Page Directory               Entry
		1c) Page Directory Pointer Table Entry
		1d) Page Map Level 4             Entry

		# Page Table Entry

			 0       Present                           bit [P]
			 1       Read/Write                        bit [R/W]
			 2       User/Supervisor          selector bit [U/S]
			 3       Write-Through/Write-Back selector bit [PWT]
			 4       Cache disable                     bit [PCD]
			 5       Accessed                          bit [A]
			 6       Dirty                             bit [D]
			 7       Page Attribute Table              bit [PAT]
			 8       Global                            bit [G]
			 9 : 11  (Available for OS use)
			12 : M-1 Physical page upper bits (12:(M-1))
			 M : 51  Reserved
			52 : 58  (Available for OS use)
			59 : 62  Protection Key                        [PK] := 0
			63       Execute disable                       [XD]

		According to osdev, if PAT is enabled, the PAT, PWT and PCD bits are
		repurposed as a three-bit index into the PAT MSR register, which offers
		more caching options, in PAT:PCD:PWT order.

		Limine furthers configures the PAT MSR which the following values:

			PAT0 -> WB
			PAT1 -> WT
			PAT2 -> UC-
			PAT3 -> UC
			PAT4 -> WP
			PAT5 -> WC
			PAT6 -> unspecified
			PAT7 -> unspecified

		Thus, the following caching combinations are possible:

			PAT = 0, PCD = 0, PWT = 0 → Write-Back
			PAT = 0, PCD = 0, PWT = 1 → Write-Through
			PAT = 0, PCD = 1, PWT = 0 → Uncacheable (overridable)
			PAT = 0, PCD = 1, PWT = 1 → Uncacheable
			PAT = 1, PCD = 0, PWT = 0 → Write-Protect
			PAT = 1, PCD = 0, PWT = 1 → Write-Combining
			PAT = 1, PCD = 1, PWT = 0 is UB
			PAT = 1, PCD = 1, PWT = 1 is UB

		Protection key support should be disabled by default.

		https://wiki.osdev.org/Paging#PAT
		https://github.com/limine-bootloader/limine/blob/v7.x/PROTOCOL.md

		# Page Directory Entry

			 0       Present                           bit [P]
			 1       Read/Write                        bit [R/W]
			 2       User/Supervisor          selector bit [U/S]
			 3       Write-Through/Write-Back selector bit [PWT]
			 4       Cache disable                     bit [PCD]
			 5       Accessed                          bit [A]
			 6       (Available for OS use)
			 7       Page Size                             [PS]  := 0
			 8 : 11  (Available for OS use)
			12 : M-1 Physical page upper bits (12:(M-1))
			 M : 51  Reserved
			52 : 62  (Available for OS use)
			63       Execute disable                  bit  [XD]

		# Page Directory Pointer Table Entry

			Identical to PDE.

		# Page Map Level 4 Entry

			Identical to PDE, but bit 7 (PS) is reserved.

		"M signifies the physical address width supported by a processor using
		PAE. Currently, up to 52 bits are supported, but the actual supported wi-
		dth may be less. "

		M can be calculated by executing the CPUID.80000008h:EAX[7:0] instruc-
		tion.

		Current layout is as follows:

			- A bunch of random mappings all over the place, known only by phys-
			map_descriptor.
			- Our kernel, at 0xfffffff800000000. Physical location given by Limine.
			- Supposedly, the HHDM should be a higher half direct physical -> virtual
			mapping given by Limine, however nothing I do makes it actually be
			in the higher half of the addressing space. Thus, we'll not use it.

		It would also seem that:

			- Each table holds a page worth of entries. With entries being eight
			bytes in size, thats 4KiB per table, or a page. In summary:

				* The (singular) PML4, with 512 entries, consumes 4KiB   of vaddr.
				* The PDPT tables,     with 512²entries, consumes 2MiB   of vaddr.
				* The PD   tables,     with 512³entries, consumes 1GiB   of vaddr.
				* The PT   tables,     with 512⁴entries, consumes 512GiB of vaddr.

		Our goal is:

			- Unmap everything from positive 128TiB region, making space for
			userspace.
			- Map the following in the negative 128TiB region:

				* Our higher half direct map (self made, not Limine's) (len 64TiB)
				  @ 0xffff'1000'0000'0000 until 0xffff'4fff'ffff'ffff

				* Our pagetables @ 0xffff'6000'0000'0000 (len ~513GiB)
				             until 0xffff'6080'4020'0fff

					→ PML4 : 0xffff'6000'0000'0000 (len 4KiB 0x1000)
					   until 0xffff'6000'0000'0fff
					→ PDPT : 0xffff'6000'0000'1000 (len 2MiB 0x200000)
					   until 0xffff'6000'0020'0fff
					→ PD   : 0xffff'6000'0020'1000 (len 1GiB)
					   until 0xffff'6000'4020'0fff
					→ PT   : 0xffff'6000'4020'1000 (len 512GiB)
					   until 0xffff'6080'4020'0fff

				* physpage stack @ 0xffff'7000'0000'0000 (len (PHYS_AMNT / 4KiB) * 8 [MAX 512GiB])
				             until 0xffff'707f'ffff'ffff

				* Our kernel     @ 0xffff'ffff'8000'0000 (len 64MiB  0x0400'0000)
				             until 0xffff'ffff'83ff'fff
				* vmalloc area   @ 0xffff'ffff'8400'0000 (len 256MiB 0x1000'0000)
				             until 0xffff'ffff'93ff'ffff

		To convert a virtual address to a physical address, you take it, subdiv-
		ive the address into the respective pagetable indices, walk it and then
		retrieve the physical address.

		"vmalloc area" is the virtual address range where all memory allocated
		for kernel use shall be put into. As the name suggest, the vmalloc() se-
		ries of calls are responsable for this region. Userspace allocations will
		be handled separately.

		"physpage stack" is where we will put the physical memory page metadata.
		As the name indicates, version 1 of Dagger will use a stack to manage
		physical memory. As per osdev, "The address of each available physical
		frame is stored in a stack-like dynamic structure. Allocating a page is
		fast (O(1)), freeing a page too but checking the state of a page is not
		practical, unless additional metadata is sorted by physical address."

		https://wiki.osdev.org/Page_Frame_Allocation

		The following functions shall define our vmalloc api:

			- void  vmalloc_enumerate(void* base, size_t count);
				This will push 'count' pages into the physpage stack,
				with the phys addr starting at base. Region must be
				contiguous.

				This function can fail silently. Exacerbate caution.

				'base' must be aligned to 4KiB. 'count' must be non-zero. The
				entire region denoted by 'base:count' must point to a valid set
				of free physical pages.

			- void* vmalloc_phys_take(void);
				This will pop a physpage from the physpage stack.

				This function will return the physical address of a free phys-
				ical page, or return NULL to indicate we're out of memory.

			- void vmalloc_phys_give(void* physaddr)
				This will push a physpage to the physpage stack.

				This function can panic() the system if too much memory is pushed.

			- void* vmalloc(size_t count)
				This function will allocate 'count' pages of physical memory and
				map them into the vmalloc area. Physical pages may be disconti-
				guous, but the virtual range will be contiguous.

				'count' must be non zero.

				This function will either return the address to a newly allocated
				memory region in the vmalloc area, or return an error.

				· ENOMEM → Not enough physical memory
				· EINVAL → Count is 0.

				Note that vmalloc area pointers are negative, such one can test
				for errors by casting as signed and checking if 'x => 0'.

			- void vfree(void* vaddr, size_t count);

				This function takes 'count' contiguous virtual pages starting
				at vaddr and frees, walking the pagetables to get the physical
				addresses.

				'vaddr:count' must point to pages allocated with
				vmalloc().

				This function panic()s the system if 'vaddr:count' points to
				alteast one page that was not allocated with vmalloc().
