- Research physical memory, virtual memory, pagetables
	* Best way to make a direct physical map
	* How to manipulate the pagetables
	* Allocators!

	1) Implement the pages themselves
		1a) Page Table                   Entry
		1b) Page Directory               Entry
		1c) Page Directory Pointer Table Entry
		1d) Page Map Level 4             Entry

		# Page Table Entry

			 0       Present                           bit [P]
			 1       Read/Write                        bit [R/W]
			 2       User/Supervisor          selector bit [U/S]
			 3       Write-Through/Write-Back selector bit [PWT]
			 4       Cache disable                     bit [PCD]
			 5       Accessed                          bit [A]
			 6       Dirty                             bit [D]
			 7       Page Attribute Table              bit [PAT]
			 8       Global                            bit [G]
			 9 : 11  (Available for OS use)
			12 : M-1 Physical page upper bits (12:(M-1))
			 M : 51  Reserved
			52 : 58  (Available for OS use)
			59 : 62  Protection Key                        [PK] := 0
			63       Execute disable                       [XD]

		According to osdev, if PAT is enabled, the PAT, PWT and PCD bits are
		repurposed as a three-bit index into the PAT MSR register, which offers
		more caching options, in PAT:PCD:PWT order.

		Limine furthers configures the PAT MSR which the following values:

			PAT0 -> WB
			PAT1 -> WT
			PAT2 -> UC-
			PAT3 -> UC
			PAT4 -> WP
			PAT5 -> WC
			PAT6 -> unspecified
			PAT7 -> unspecified

		Thus, the following caching combinations are possible:

			PAT = 0, PCD = 0, PWT = 0 → Write-Back
			PAT = 0, PCD = 0, PWT = 1 → Write-Through
			PAT = 0, PCD = 1, PWT = 0 → Uncacheable (overridable)
			PAT = 0, PCD = 1, PWT = 1 → Uncacheable
			PAT = 1, PCD = 0, PWT = 0 → Write-Protect
			PAT = 1, PCD = 0, PWT = 1 → Write-Combining
			PAT = 1, PCD = 1, PWT = 0 is UB
			PAT = 1, PCD = 1, PWT = 1 is UB

		Protection key support should be disabled by default.

		https://wiki.osdev.org/Paging#PAT
		https://github.com/limine-bootloader/limine/blob/v7.x/PROTOCOL.md

		# Page Directory Entry

			 0       Present                           bit [P]
			 1       Read/Write                        bit [R/W]
			 2       User/Supervisor          selector bit [U/S]
			 3       Write-Through/Write-Back selector bit [PWT]
			 4       Cache disable                     bit [PCD]
			 5       Accessed                          bit [A]
			 6       (Available for OS use)
			 7       Page Size                             [PS]  := 0
			 8 : 11  (Available for OS use)
			12 : M-1 Physical page upper bits (12:(M-1))
			 M : 51  Reserved
			52 : 62  (Available for OS use)
			63       Execute disable                  bit  [XD]

		# Page Directory Pointer Table Entry

			Identical to PDE.

		# Page Map Level 4 Entry

			Identical to PDE, but bit 7 (PS) is reserved.

		"M signifies the physical address width supported by a processor using
		PAE. Currently, up to 52 bits are supported, but the actual supported wi-
		dth may be less. "

		M can be calculated by executing the CPUID.80000008h:EAX[7:0] instruc-
		tion.

		Current layout is as follows:

			- A bunch of random mappings all over the place, known only by phys-
			map_descriptor.
			- Our kernel, at 0xfffffff800000000. Physical location given by Limine.
			- Supposedly, the HHDM should be a higher half direct physical -> virtual
			mapping given by Limine, however nothing I do makes it actually be
			in the higher half of the addressing space. Thus, we'll not use it.

		Our goal is:

			- Unmap everything from positive 128TiB region, making space for
			userspace.
			- Map the following in the negative 128TiB region:

				* Our higher half direct map (self made, not Limine's) (len 64TiB)
				  @ 0xffff'1000'0000'0000 until 0xffff'4fff'ffff'ffff

				* physpage stack @ 0xffff'6000'0000'0000 (len (PHYS_AMNT / 4KiB) * 8 [MAX 512GiB])
				             until 0xffff'607f'ffff'ffff

				* Our kernel     @ 0xffff'ffff'8000'0000 (len 64MiB  0x0400'0000)
				             until 0xffff'ffff'83ff'fff
				* vmalloc area   @ 0xffff'ffff'8400'0000 (len 256MiB 0x1000'0000)
				             until 0xffff'ffff'93ff'ffff

		To convert a virtual address to a physical address, you take it, subdiv-
		ive the address into the respective pagetable indices, walk it and then
		retrieve the physical address.

		"vmalloc area" is the virtual address range where all memory allocated
		for kernel use shall be put into. As the name suggest, the vmalloc() se-
		ries of calls are responsable for this region. Userspace allocations will
		be handled separately.

		"physpage stack" is where we will put the physical memory page metadata.
		As the name indicates, version 1 of Dagger will use a stack to manage
		physical memory. As per osdev, "The address of each available physical
		frame is stored in a stack-like dynamic structure. Allocating a page is
		fast (O(1)), freeing a page too but checking the state of a page is not
		practical, unless additional metadata is sorted by physical address."

		https://wiki.osdev.org/Page_Frame_Allocation

		The following functions shall define our paging api:

			- void  pages_apply(void);
				This function will make visible to the system changes
				made to the pagetables by invalidating the TLB. Note that the chan-
				ges may be visible prior to this call in the rare case the TLB inva-
				lidates itself.

			- void  pages_map  (void* phys_addr, void* vaddr);
				Maps physical 'phys_addr' → virtual 'vaddr'

			- void  pages_unmap(void* vaddr);
				Undos virtual 'vaddr'.

			- void* pages_peek(void* vaddr);
				This function will check the pagetable entry that populates virtual address
				'vaddr' and return it's physical address. It shall return NULL if 'vaddr' is not
				mapped.

			- void* pages_getentry(void* vaddr)
				This function will get the virtual address of the entry that maps to 'vaddr'. If
				'vaddr' is not mapped then it will return NULL.

			- int   pages_walk(void* entry, struct pagewalk_result* result)
				This function will walk the pagetables, starting at page entry address 'entry', then:

				* Return if there is a next entry.
				 · 0      is no next entry
				 · -1     if    next entry
				 · EINVAL if 'vaddr' points to nowhere.

				* Information about the current entry will be available in 'result'.

				Setting 'result' to NULL is undefined behaviour.

				// TODO: define struct pagewalk_result, thought atleast phys_addr and the pointer to the
				         next entry will be there. There must be a pointer to the next entry as the page-
				         tables are not necessarily contiguous.


		The following functions shall define our physpage api:
			
			- void  phys_enumerate(void* base, size_t count);
				This will push 'count' pages into the physpage stack,
				with the phys addr starting at base. Region must be
				contiguous.

				This function can fail silently. Exacerbate caution.

				'base' must be aligned to 4KiB. 'count' must be non-zero. The
				entire region denoted by 'base:count' must point to a valid set
				of free physical pages.

			- void* phys_pop(void)
				This will pop a physpage from the physpage stack.

				This function will return the physical address of a free phys-
				ical page, or return NULL to indicate we're out of memory.

			- void phys_push(void* physaddr)
				This will push a physpage to the physpage stack.

				'physaddr' should be a page acquired with phys_pop(), doing other-
				wise is undefined behaviour.

				This function can panic() the system if too much memory is pushed.


		The following functions shall define our vmalloc api:

			- void* vmalloc(size_t count)
				This function will allocate 'count' pages of physical memory and
				map them into the vmalloc area. Physical pages may be disconti-
				guous, but the virtual range will be contiguous.

				'count' must be non zero.

				This function will either return the address to a newly allocated
				memory region in the vmalloc area, or return an error.

				· ENOMEM → Not enough physical memory
				· EINVAL → Count is 0.

				Note that vmalloc area pointers are negative, such one can test
				for errors by casting as signed and checking if 'x => 0'.

			- void vfree(void* vaddr, size_t count);

				This function takes 'count' contiguous virtual pages starting
				at vaddr and frees, walking the pagetables to get the physical
				addresses.

				'vaddr:count' must point to pages allocated with
				vmalloc().

				This function panic()s the system if 'vaddr:count' points to
				alteast one page that was not allocated with vmalloc().
